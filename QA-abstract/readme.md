# QA生成对工作集

本项目包含四个独立但相互配合的Python脚本，主要用于实验报告的文本处理、问答对生成、问题自动回答及文本拆分，面向科研场景中的文档自动摘要、问答生成和文本预处理。以下是各脚本的功能介绍及使用说明。

---

## 目录结构

- `agent_for_.py`  
  实验报告多阶段板块提取与对比分析主智能体。  
  从指定目录读取多份实验报告（.docx格式），提取文本后调用本地LLM接口生成每个板块的总结，再对不同阶段报告同一板块的总结进行对比分析，最终输出JSON格式的问答对。

- `QA-from-text-multiQA.py`  
  根据指定文本内容，调用远程大模型接口自动生成多条多样化且高质量的问答对（Q&A pairs），适合用于问答数据增强或人工评估。支持处理.md和.pdf文件，自动提取文本内容。

- `QA-from_q.py`  
  根据给定的预设问题列表，批量调用大模型接口获取问题答案，支持并行处理，提高批量问答生成效率。结果保存为jsonl文件，适合快速针对固定问题集进行自动问答。

- `split.py`  
  文本拆分工具，将较长的Markdown文档根据一级标题分割成多个小块，方便分批送入模型处理，避免单次文本过长。支持批量处理目录下所有.md文件，并输出拆分后的小文档。
- output中存放了几个对应的脚本跑出来的结果仅供参考
---

## 环境依赖

- Python 3.8+  
- 依赖库（可通过`pip install -r requirements.txt`安装）：  
  - requests  
  - python-docx  
  - PyPDF2  
  - tqdm  
  - openai （`QA-from_q.py`中使用）  

---

## 使用说明

## 1. `agent_for_.py` — 实验报告多阶段总结与对比分析

- 修改 `REPORTS_BASE_DIR` 为你的实验报告存放目录，报告需为 `.docx` 格式。  
- 运行脚本后，程序将自动提取所有报告文本，针对预定义的板块名称（如“放电统计”、“主要结果”等）生成总结。  
- 随后对不同报告阶段中同一板块的内容进行对比分析。  
- 最终输出包含问答对的JSON文件，文件路径和时间戳可在代码中查看。

###  参数说明

| 参数名            | 说明                                   | 作用                                   | 示例                       |
| ----------------- | ------------------------------------ | ------------------------------------ | -------------------------- |
| `REPORTS_BASE_DIR` | 存放实验报告 `.docx` 文件的根目录路径 | 程序扫描该目录下所有报告文件进行提取和分析 | `./reports/2025/`          |
| `BLOCK_NAMES`      | 指定要提取和总结的报告板块名称列表     | 只针对这些板块进行内容抽取和总结       | `["放电统计", "主要结果", "讨论"]` |
| `LLM_API_ENDPOINT` | 本地或远程大模型接口地址（可选）       | 用于调用模型生成总结和对比文本         | `"http://localhost:8000/api"` |
| `OUTPUT_DIR`       | 结果输出目录                           | 保存JSON问答文件                      | `./output/`                |


```bash
python agent_for_.py
```
---


## 2. `QA-from-text-multiQA.py` — 文本驱动的多问答对生成

- 修改脚本中 `folder_path` 为存放待处理 `.md` 或 `.pdf` 文件的目录。  
- 运行后，针对目录下每个文件调用远程模型生成多条问答对，保存为JSONL文件。  
- 支持文本截断，避免超长文本影响请求。

### 参数说明

| 参数名             | 说明                                    | 作用                                  | 示例                      |
| ------------------ | ------------------------------------- | ----------------------------------- | ------------------------- |
| `folder_path`      | 存放待处理 `.md` 或 `.pdf` 文件的目录路径 | 程序依次处理目录内所有文件进行文本提取和问答生成 | `./data/docs_to_qa/`      |
| `max_tokens`       | 单次调用模型生成问答对时最大token数（可选） | 控制请求长度，避免超过模型限制       | `1024`                    |
| `num_questions_per_file` | 每个文件生成问答对数量（可选）           | 调节输出问答对丰富度                 | `5`                       |
| `output_path`      | 生成问答对保存的JSONL文件路径            | 保存生成的问答对数据       
```bash
python QA-from-text-multiQA.py
```
---
## 3. QA-from_q.py — 预设问题批量问答生成
将待回答的问题保存为JSONL文件，文件中每行包含 input 字段。

修改脚本中 question_file 为问题文件路径，output_file 为输出结果路径。

运行后批量并行调用大模型接口获取答案，结果存为JSONL。


##   参数说明

| 参数名             | 说明                                   | 作用                                  | 示例                      |
| ------------------ | ------------------------------------ | ----------------------------------- | ------------------------- |
| `question_file`    | 包含待回答问题的JSONL文件路径           | 读取问题，批量调用模型生成答案       | `./questions/questions.jsonl` |
| `output_file`      | 保存回答结果的JSONL文件路径              | 存储批量生成的答案                   | `./output/answers.jsonl`  |
| `parallel_workers` | 并行调用接口的线程或进程数（可选）       | 提升批量问答生成速度                 | `4`                       |
| `model_name`       | 调用的大模型名称或标识（可选）           | 指定模型版本     
```
bash

python QA-from_q.py
```
---
## 4. split.py — Markdown文档拆分工具
修改脚本中 input_dir 为待拆分的Markdown文件目录，output_dir 为拆分后文件的存放目录。

运行后将自动将每个 .md 文件按一级标题拆分成多个块，每块包含若干章节，便于后续模型分批处理。
### 参数说明

| 参数名         | 说明                          | 作用                                  | 示例                      |
| -------------- | ----------------------------- | ----------------------------------- | ------------------------- |
| `input_dir`    | 待拆分Markdown文件所在目录路径   | 批量处理目录下所有 `.md` 文件           | `./data/markdowns/`       |
| `output_dir`   | 拆分后文件的保存目录             | 拆分后小文档存储位置                   | `./output/split_docs/`    |
| `split_level`  | 拆分标题级别，默认一级标题(`#`) | 控制拆分的颗粒度                       | `1`                       |
| `max_chunk_size` | 每块最大字符数限制（可选）       | 避免单块文本过长导致模型处理失败        | `2000`                    |
```
bash

python split.py
```
### 典型工作流程示例
使用 split.py 将大型Markdown文档拆分成多个小块，方便模型批量处理。

使用 QA-from-text-multiQA.py 对拆分后的文本块生成丰富的问答对数据。

使用 QA-from_q.py 对预设的问题集进行批量答案生成。

使用 agent_for_.py 对多个阶段的实验报告内容进行自动提取、总结和对比分析，形成结构化问答结果。

---

# 使用注意事项



## 1. 模型Token限制

- **Token上限**  
  大多数语言模型对单次请求的Token数有限制（如GPT-3.5通常限制4096 Token，GPT-4不同版本有不同限制）。  
  - 这里的Token包括输入的文本和模型生成的输出文本的总和。  
  - 超过限制会导致请求失败或截断输出。  

- **解决方案**  
  - 合理拆分长文本，避免一次输入过大。  
  - 设置合理的 `max_tokens` 参数，控制生成长度。  
  - 使用 `split.py` 先拆分Markdown文本成小块，再分别调用模型。

---

## 2. 文件格式与编码

- 脚本支持 `.docx`（`agent_for_.py`）、`.md` 和 `.pdf` 文件（`QA-from-text-multiQA.py`）。  
- 确保文件格式正确且不损坏，否则可能导致解析失败。  
- 文件编码建议使用UTF-8，避免乱码。

---

## 3. 并行调用限制

- 并行调用接口（如`QA-from_q.py`的`parallel_workers`）虽可提高速度，但会增加接口压力。  
- 受限于API并发限制和网络带宽，过高并发可能导致请求失败。  
- 建议根据实际API限制和服务器性能合理调整并发数。

---

## 4. 目录和路径规范

- 参数中指定的路径（如`REPORTS_BASE_DIR`、`folder_path`等）应为有效的绝对路径或相对路径。  
- 确保程序有该目录的读写权限。  
- 输出目录若不存在，程序通常会尝试创建，建议提前确认权限和磁盘空间。

---

## 5. 网络环境要求

- 调用远程大模型API时需确保网络连接稳定。  
- 如果使用本地模型接口，确认本地服务启动并正确配置端口。

---

## 6. 资源消耗

- 运行文本处理和大模型调用时，可能会占用较多CPU、内存和网络带宽。  
- 对于大规模文件和批量处理，建议在性能较好的机器或服务器上运行。

---

## 7. 日志与错误处理

- 脚本一般会输出运行日志，建议关注错误信息。  
- 遇到解析失败或调用失败时，检查文件格式、网络连接和API配额。  
- 适当加上异常处理代码提高健壮性。

---

## 8. 参数调整建议

- 根据文本大小和复杂度，调整`max_tokens`、`num_questions_per_file`、`max_chunk_size`等参数，保证生成质量和稳定性。  
- 尽量避免一次调用过大文本，导致请求超时或失败。

---

如果你需要，我可以帮你写一份详细的“FAQ及故障排查”章节，或者给出常见错误案例及解决办法。需要吗？  
